---
title: "STAT/MATH 495: Problem Set 06"
author: "Jonathan Che"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(tidyverse)
library(broom)
library(knitr)
```

# Collaboration

Worked a bit with Sarah T.

# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x),
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  
  return(sample)
}
```

Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sample <- 10000
```


# Computation

```{r}
# Initialize empty vectors to store model predictions & simulated x0 values
preds_df2 <- c()
preds_df99 <- c()
sample_y <- c()
for (i in 1:n_sample){
  # Generate random sample of points (under given parameters)
  samp <- generate_sample(f, n, sigma)
  sample_y <- c(sample_y, mean(samp$y))
  
  # Fit two models to points
  m_df2 <- smooth.spline(x=samp$x, y=samp$y, df = 2)
  m_df99 <- smooth.spline(x=samp$x, y=samp$y, df = 99)
  
  # Get model predictions for x=x0=0.95
  pred_df2 <- unlist(predict(m_df2, x=x0)[2])
  pred_df99 <- unlist(predict(m_df99, x=x0)[2])
  
  # Add new model prediction to list of predictions
  preds_df2 <- c(preds_df2, pred_df2)
  preds_df99 <- c(preds_df99, pred_df99)
}

```


# Tables

```{r, warning=FALSE}
true <- rep(f(x0), n_sample)
true_noisy <- true + rnorm(n = n_sample, mean = 0, sd = sigma)
df <- data.frame(preds_df2, preds_df99, true, true_noisy)
df <- df %>%
  gather(df, pred, preds_df2:preds_df99)
df %>%
  group_by(df) %>%
  summarize(
    MSE = mean((true_noisy - pred)^2),
    `Bias^2` = (mean(pred) - mean(true))^2,
    Variance = mean((pred - mean(pred))^2),
    Irreducible = sigma^2
  ) %>%
  mutate(
    Sum = `Bias^2` + Variance + Irreducible
  ) %>%
  knitr::kable(digits=4)
```

# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.
2. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all*
$x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.
3. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1. One possible "sanity check" would be seeing if changing $\sigma$? Plotting/visualizing? 
2. The simulation procedure would be similar, except model predictions would be made on all the $x$ in the simulated sample (and not just at $x_0 = 0.95$ ) and MSE, Bias^2, and Variance would be computed based on all of the points (i.e. MSE computed as it usually is, bias and variance computed over all $x$).
3. I would use the first model, with 2 degrees of freedom, to predict the point of interest. Even though it is more biased, it is also much less variable. Thus, the predictions it makes for points of interest $x_0$ can be reasonably expected to be close to $f(x_0) + b$, where $f(x)$ is the true function and $b$ is some amount of bias. This bias can be characterized based on prior observations. Even if we do not do so, we can still have confidence that a single prediction made by the model will be very close to a biased estimate of the true model, as opposed to having no confidence that any single prediction made by the model is particularly close to the true model.

